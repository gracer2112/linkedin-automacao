#analise_vaga_ia.py
import sys
from dotenv import load_dotenv
import os
import json
import logging
import time

import google.generativeai as genai
from google.generativeai import types
from google.auth import default # Para carregar credenciais automaticamente
import pandas as pd

# ================= LOGGING SETUP =================
loglevel = os.environ.get("MY_LOG_LEVEL", "INFO").upper()
logging.basicConfig(
    level=loglevel,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# ==== PASSO 1: Carregar vari√°veis de ambiente seguras ====
load_dotenv()  # Carrega automaticamente as vari√°veis do .env

def carrega_chave():
    gemini_api_key = os.getenv('GEMINI_API_KEY')
    if not gemini_api_key:
        log_erro("Chave de API do Google n√£o encontrada (GEMINI_API_KEY).")
        sys.exit(1)
        
    genai.configure(api_key=gemini_api_key)
    return genai


def log_erro(msg):
    """Escreve um erro formatado no stderr (como JSON)."""
    sys.stderr.write(json.dumps({"error": str(msg)}, ensure_ascii=False, indent=2) + '\n')
    sys.stderr.flush()

# Carregando configura√ß√µes do linkedin.json
def carregar_configuracoes_json(caminho_config='configs/linkedin.json'):
    try:
        with open(caminho_config, 'r', encoding='utf-8') as f:
            configs = json.load(f)
        return configs
    except Exception as e:
        log_erro(f"Erro ao carregar configura√ß√µes: {e}")
        sys.exit(1)

# ==== PASSO 4: Salvar resultados e erros em JSON ====
def salvar_json(dados, caminho):
    try:
        with open(caminho, 'w', encoding='utf-8') as f:
            json.dump(dados, f, ensure_ascii=False, indent=2)
    except Exception as e:
        log_erro(f"Erro ao salvar JSON ({caminho}): {e}")

# ==== PASSO 3: Fun√ß√£o para ler dados ====
def ler_vagas_do_excel(arquivo_excel, coluna_visualizado="Visualizado"):
    try:
        if not os.path.isfile(arquivo_excel):
            raise FileNotFoundError(f"Arquivo {arquivo_excel} n√£o encontrado.")
        df = pd.read_excel(arquivo_excel)

        if coluna_visualizado in df.columns:
            df_visualizado = df[df[coluna_visualizado].isna() | (df[coluna_visualizado] == "")].copy()
        else:
            df_visualizado = df  # Se n√£o existir, processa tudo

        #df_visualizado = df_visualizado.head(2) #comentar ap√≥s testar
        return df_visualizado  # DataFrame 
    except Exception as e:
        log_erro(f"Erro ao ler o Excel: {e}")
        sys.exit(1)

# ==== PASSO 2: Fun√ß√£o para an√°lise de vaga (usando a API do Gemini) ====
def analisar_vaga(genai, texto_vaga, indice,codigo,modelo="gemini-1.5-flash"):
#def analisar_vaga(genai, texto_vaga, indice,codigo,modelo="gemini-2.5-flash-preview-05-20"):
    result = None

    prompt = f"""
    Aja como um analista de RH.
    Analise a seguinte descri√ß√£o de vaga e extraia as informa√ß√µes solicitadas no formato JSON.
    O JSON deve ter a seguinte estrutura e incluir apenas as chaves mencionadas:
    {{
    "titulo": "",
    "localizacao": "",
    "senioridade": "",
    "requisitos_obrigatorios": [],
    "requisitos_desejaveis": [],
    "soft_skills": [],
    "hard_skills": []
    }}
    Descri√ß√£o da vaga:
    {texto_vaga}
    """
    generate_text=""

    if pd.isna(texto_vaga) or not isinstance(texto_vaga, str) or texto_vaga.strip() == "":
        log_erro(f"Descri√ß√£o da vaga {indice+1} {codigo} est√° vazia ou inv√°lida. Pulando.")
        return None, "vaga vazia ou inv√°lida"
               
    try:
        modelo = genai.GenerativeModel(modelo)
        logging.info(f"\nAnalisando a vaga {indice + 1}  {codigo}")
    except Exception as e:
        log_erro(f"Erro ao carregar o modelo: {e}")
        return None, f"Erro ao carregar o modelo: {e}"

    try:
        # Chamada para gerar o conte√∫do
        config =  genai.types.GenerationConfig(temperature=0.7,response_mime_type="application/json")
        result = modelo.generate_content(
            prompt,  # Usando a nova API com Part
            generation_config=config
            )
    except Exception as e:
        log_erro(f"Erro ao chamar a API do modelo: {e}")
        return None, f"Erro ao chamar o modelo: {e}"

    # Limpa o texto removendo espa√ßos extras e quebras de linha
    texto_limpo = result.text.strip()

    # Remover os marcadores de bloco de c√≥digo Markdown
    if texto_limpo.startswith("```json"):
        texto_limpo = texto_limpo[len("```json"):]
    if texto_limpo.startswith("```"): # Caso seja apenas ```
        texto_limpo = texto_limpo[len("```"):]
    if texto_limpo.endswith("```"):
        texto_limpo = texto_limpo[:-len("```")]
    
    # Remove markdown code blocks (tolerando espa√ßamentos)
    texto_limpo = texto_limpo.replace("```json", "").replace("```", "").strip()

    try:
        # Certifique-se de que cleaned_text n√£o est√° vazio ap√≥s a limpeza
        if not texto_limpo:
            raise json.JSONDecodeError("String vazia ap√≥s limpeza dos marcadores", "", 0)
        
        dados_json = json.loads(texto_limpo)

        return dados_json, None

    except json.JSONDecodeError as e:
        log_erro(f"Erro ao decodificar JSON ap√≥s strip para a vaga {indice} {codigo}: {e}")
        #log_erro(f"Conte√∫do problem√°tico (ap√≥s strip): {repr(texto_limpo)}")
        return None, f"Erro ao decodificar JSON: {e}"

    except Exception as e:
        log_erro(f"Erro inesperado ao processar a vaga {indice} {codigo}: {e}")
        return None, f"Erro inesperado: {e}"

# ==== PASSO 5: Fun√ß√£o principal de processamento ====
def processar_todas_as_vagas_excel(config_cam):
    # L√™ configura√ß√µes
    arquivo_entrada = config_cam['input_file_jobs']
    arquivo_saida = config_cam['output_file_requirements']
    arquivo_erro = config_cam.get('output_file_error_requirements', "erros_analise_vagas.json")
    coluna_descricao = config_cam.get('col_linkedin_job_description', "Description")
    coluna_codigo = config_cam.get('col_linkedin_job_code', "Code")
    coluna_visualizado = config_cam.get('col_linkedin_job_visualizado', "Visualizado")   

    df = ler_vagas_do_excel(arquivo_entrada, coluna_visualizado)
    resultados_analise = []
    erros_analise = []

    genai=carrega_chave()

    for idx, row in df.iterrows():
        texto_vaga = row.get(coluna_descricao, "")
        logging.info(f"Processando vaga {idx+1} de {len(df)}...")
        resultado, erro = analisar_vaga(genai, texto_vaga,idx, row.get("Code"))
        vaga_dict = row.to_dict()

        code = vaga_dict.get("Code")
        if pd.notna(code) and code != "":
            try:
                codigo_final = str(int(float(code)))
            except (ValueError, TypeError):
                codigo_final = str(code)
        else:
            codigo_final = ""        

        ref = {
            "Code": codigo_final,
            "Company": vaga_dict.get("Company") if pd.notna(vaga_dict.get("Company")) else "",
            "Link": vaga_dict.get("Link") if pd.notna(vaga_dict.get("Link")) else "",
        }

        if resultado is not None:
            resultados_analise.append({
                "analise": resultado,
                "referencia": {
                "Code": ref["Code"],
                "Company": ref["Company"],
                "Link": ref["Link"],
                }
            })
        else:
            log_erro(f"Erro na vaga {idx+1}: {erro}")
            erros_analise.append({
                "Code": ref["Code"],
                "erro": erro
            })
            
        time.sleep(1)  # Respeita limites de API

    salvar_json(resultados_analise, arquivo_saida)
    if erros_analise:
        salvar_json(erros_analise, "erros_analise_vagas.json")
        logging.info(f"\n{len(resultados_analise)} vagas processadas com sucesso.")
        logging.info(f"{len(erros_analise)} vagas tiveram erro. Veja 'erros_analise_vagas.json'.")
    else:
        logging.info(f"\nSucesso! Todas as {len(resultados_analise)} vagas processadas.")
        # n8n espera sa√≠da via STDOUT
    return resultados_analise        

# ==== PASSO 6: Execu√ß√£o Principal ====
if __name__ == "__main__":
    # Entrada do n8n: caminho do JSON de config
    config_path = os.environ.get('CONFIG_JSON_PATH', 'configs/linkedin.json')
    config = carregar_configuracoes_json(config_path) 
    saida=processar_todas_as_vagas_excel(config)


    # Retorno apropriado para n8n
    print(json.dumps(saida, ensure_ascii=False, indent=2))


#aderencia_cv_vaga_ia.py
from dotenv import load_dotenv
import sys
import os
import select
import logging

load_dotenv()  # Carrega as vari√°veis do .env

# ================= LOGGING SETUP =================
loglevel = os.environ.get("MY_LOG_LEVEL", "INFO").upper()
logging.basicConfig(
    level=loglevel,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

def checa_credenciais_google():
    cred_path = os.environ.get("GOOGLE_APPLICATION_CREDENTIALS")
    if not cred_path or not os.path.exists(cred_path):
        msg = (
            "ERRO: Vari√°vel de ambiente GOOGLE_APPLICATION_CREDENTIALS n√£o est√° definida ou o arquivo n√£o existe.\n"
            "Defina a vari√°vel antes de rodar o programa.\n"
            "\n"
            "Exemplo no terminal Linux/Mac:\n"
            'export GOOGLE_APPLICATION_CREDENTIALS="/caminho/para/sua/credencial.json"\n'
            "Exemplo em c√≥digo (N√ÉO recomendado para produ√ß√£o):\n"
            'os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "/caminho/para/sua/credencial.json"\n'
        )
        sys.stderr.write(msg)
        sys.exit(1)
    else:
         logging.info(f"üí° Usando credenciais do arquivo: {cred_path}")

# Use sempre este check ANTES de importar qualquer SDK Google:
checa_credenciais_google()

import json
import vertexai
from vertexai.language_models import TextEmbeddingModel
import google.generativeai as genai
from google.generativeai import types
from google.auth import default # Para carregar credenciais automaticamente

from docx import Document
import numpy as np


def stdin_has_data():
    return not sys.stdin.isatty() and select.select([sys.stdin], [], [], 0.1)[0]

def ler_config(path_config):
    with open(path_config, 'r', encoding='utf-8') as f:
        config = json.load(f)
    return config

def extrair_texto_docx(cv_file):
    # Abre o documento
    document = Document(cv_file)

    return "\n".join([p.text for p in document.paragraphs])

def criar_embedding_batch(texto,modelo_nome):

    PROJECT_ID = os.environ.get('VERTEX_PROJECT', 'n8n-automatizando-ia-424719')
    REGION = os.environ.get('VERTEX_REGION', 'us-central1')

    vertexai.init(project=PROJECT_ID, location=REGION)
    # Inicializa o modelo de embedding
    modelo = TextEmbeddingModel.from_pretrained(modelo_nome)

    # Verifica se a entrada √© uma string para sabermos como retornar o resultado
    is_single_item = isinstance(texto, str)

    # A API sempre espera uma lista, ent√£o garantimos isso aqui.
    # Se for uma string, a transformamos em uma lista com um √∫nico item.
    # Se j√° for uma lista, a usamos diretamente.
    payload = [texto] if is_single_item else texto

    # Caso uma lista vazia seja passada, retorna uma lista vazia para evitar erro na API
    if not payload:
        return []

    # Gera o embedding para o texto do CV
    try:
        embeddings_response = modelo.get_embeddings(payload)
        # Extrai os valores num√©ricos de cada objeto de embedding retornado
        vetores = [emb.values for emb in embeddings_response]

        # Se a entrada original era uma string, retorna apenas o primeiro (e √∫nico) vetor.
        # Caso contr√°rio, retorna a lista completa de vetores.
        if is_single_item:
            return vetores[0]
        else:
            return vetores

    except Exception as e:
        raise RuntimeError(f"Erro ao gerar o embedding: {e}")


def calcular_similaridade(vetor_a, vetor_b):
    return float(np.dot(vetor_a, vetor_b) / (np.linalg.norm(vetor_a) * np.linalg.norm(vetor_b) + 1e-8))

# Adapte a fun√ß√£o comparar_cv_vagas para receber c√≥digo e descri√ß√£o
def comparar_cv_vagas(cv_texto, vagas, modelo_embedding="text-multilingual-embedding-002", codigo_col="Code", descricao_col="Job Description"):

    resultado_ranking = []
    emb_cv = criar_embedding_batch(cv_texto, modelo_embedding)

    sys.stderr.write(f"Total de vagas: {len(vagas)}\n")

    for idx, vaga in enumerate(vagas):
        codigo = vaga['referencia'].get('Code')
        sys.stderr.write(f"C√≥digo da vaga {idx}: {codigo}\n")

        analise = vaga.get("analise", {}) 
        referencia = vaga.get('referencia', {})
        if not analise:  # Pula se o item n√£o tiver a chave "vaga"
            logging.info(f"[IGNORADA] Vaga {codigo}: sem campo 'analise'.", file=sys.stderr)

        # Pega a descri√ß√£o usando o nome da coluna din√¢mico do seu JSON
        descricao = analise.get(descricao_col, "")
        logging.info("VAGA RECEBIDA:", vaga)
        logging.info("CHAVES:", vaga.keys())

        # Adapte para obter requisitos obrigat√≥rios/desej√°veis
        #reqs = vaga.get("requisitos_obrigatorios", []) + vaga.get("requisitos_desejaveis", [])
        obrigatorios = analise.get('requisitos_obrigatorios', [])
        desejaveis   = analise.get('requisitos_desejaveis', [])
        reqs = obrigatorios + desejaveis

        if not reqs:
            logging.info(f"[IGNORADA] Vaga {codigo}: sem requisitos obrigat√≥rios nem desej√°veis.", file=sys.stderr)
            continue

        logging.info(f"[PROCESSADA] Vaga {codigo}: processando normalmente.", file=sys.stderr)

        emb_reqs = criar_embedding_batch(reqs, modelo_embedding)
        logging.info("embedding retornado reqs:", emb_reqs)

        scores = [calcular_similaridade(emb_cv, emb_req) for emb_req in emb_reqs]

        if not scores: # Seguran√ßa extra para evitar divis√£o por zero
            continue

        detalhes = [
            {"requisito": req, "score": round(score, 4)}
            for req, score in zip(reqs, scores)
        ]
        score_geral = round(sum(scores) / len(scores), 4)
        resultado_ranking.append({
            "codigo": referencia.get(codigo_col, ""),
            "similaridade_geral": score_geral,
            "detalhes": detalhes
        })

    resultado_ranking.sort(key=lambda x: x["similaridade_geral"], reverse=True)
    return resultado_ranking


def main():

    config_path = os.environ.get('CONFIG_JSON_PATH', 'configs/linkedin.json')    
    config = ler_config(config_path)

    # L√™ configura√ß√µes
    vagas_json_path  = config['output_file_requirements']
    cv_docx_path  = config['input_file_cv']
    diretorio_saida = config['output_file_score']

    # Extrai texto do CV (.docx)
    cv_texto = extrair_texto_docx(cv_docx_path)

    # L√™ as vagas do JSON no n8n como a entrada √© via stdin a linha de baixo n√£o √© necesspara
    if stdin_has_data():
        entrada_json = sys.stdin.read()
    elif len(sys.argv) > 1:
        with open(sys.argv[1]) as f:
            entrada_json = f.read()
    else:
        logging.info("Nenhuma entrada fornecida! Use argumento de arquivo ou STDIN.")
        exit(1)

    vagas = json.loads(entrada_json)

    print("Tipo de vagas:", type(vagas), file=sys.stderr)
    print("Primeiro item de vagas:", vagas[0] if isinstance(vagas, list) else vagas, file=sys.stderr)

    # Se n√£o √© lista, faz virar lista
    if isinstance(vagas, dict):
        vagas = [vagas]
    elif not isinstance(vagas, list):
        raise ValueError("Formato de entrada inv√°lido: deve ser lista ou objeto com chaves 'analise'/'referencia'.")

    ranking = comparar_cv_vagas(cv_texto, vagas)

    # Salva no arquivo
    with open(diretorio_saida, "w", encoding="utf-8") as f:
        f.write(json.dumps(ranking, ensure_ascii=False, indent=2))

    return ranking

if __name__ == '__main__':
    try:
        resultado = main()
        print(json.dumps(resultado, ensure_ascii=False))
    except Exception as e:
        sys.stderr.write(json.dumps({"error": str(e)},ensure_ascii=False, indent=2), file=sys.stderr)
        sys.exit(1)    
		
# cv_sugestor.py

import sys
from dotenv import load_dotenv
import os
import json
from tenacity import retry, stop_after_attempt, wait_exponential, before_log
import logging
import ast
import re
import time

import google.generativeai as genai
from google.generativeai import types
from google.auth import default # Para carregar credenciais automaticamente

from docx import Document # Ainda precisamos disso para extrair texto do CV

# ================= LOGGING SETUP =================
loglevel = os.environ.get("MY_LOG_LEVEL", "INFO").upper()
logger = logging.getLogger(__name__)
logger.setLevel(loglevel)

if not logger.handlers:
    handler = logging.StreamHandler(sys.stderr)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)



# ================= FUN√á√ïES DO SCRIPT =================

def carrega_chave():
    """Carrega a chave da API do Gemini das vari√°veis de ambiente."""
    gemini_api_key = os.getenv('GEMINI_API_KEY')
    if not gemini_api_key:
        logger.error("Chave de API do Google n√£o encontrada (GEMINI_API_KEY).")
        sys.exit(1)
    genai.configure(api_key=gemini_api_key)
    return genai

def stdin_has_data():
    """Verifica se h√° dados na entrada padr√£o (stdin)."""
    import select
    if sys.stdin.isatty():
        return False
    rlist, _, _ = select.select([sys.stdin], [], [], 0)
    return bool(rlist)

def log_custom_before_sleep(retry_state):
    """Fun√ß√£o que loga uma mensagem customizada antes de cada tentativa."""
    delay = retry_state.next_action.sleep
    logger.info(f"API retornou erro ou resposta inv√°lida. Tentando novamente em {delay:.1f} segundos...")

def interpretar_resposta_ia(resposta_texto):
    """
    Interpreta e limpa a resposta da IA para garantir que seja um JSON v√°lido.
    Lida com formata√ß√µes extras como '```json' e garante que a sa√≠da seja uma lista.
    """
    logging.warning("Resposta da IA n√£o parece ser um JSON puro. Validando entrada.")

    # Remove marca√ß√µes de c√≥digo e espa√ßos em branco desnecess√°rios
    cleaned_text = re.sub(r'```json\s*|```', '', resposta_texto).strip()

    if not cleaned_text:
        logging.warning("Resposta da IA estava vazia ap√≥s a limpeza.")
        return []

    try:
        dados_convertidos = ast.literal_eval(cleaned_text)

        if isinstance(dados_convertidos, list):
            logging.info("Resposta da IA interpretada como uma lista de sugest√µes.")
            return dados_convertidos
        elif isinstance(dados_convertidos, dict):
            if 'sugestoes' in dados_convertidos and isinstance(dados_convertidos['sugestoes'], list):
                logging.info("Resposta da IA interpretada como um dicion√°rio. Extraindo a lista 'sugestoes'.")
                return dados_convertidos['sugestoes']
            else:
                raise ValueError(f"Dicion√°rio JSON n√£o cont√©m a chave 'sugestoes' ou o valor n√£o √© uma lista. Resposta recebida: {cleaned_text}")                
        else:
            raise ValueError(f"Resposta da IA n√£o √© uma lista ou dicion√°rio v√°lido. Tipo recebido: {type(dados_convertidos)}")
            
    except (ValueError, SyntaxError, TypeError) as e:
        logging.error(f"Erro de parsing ao interpretar a resposta da IA: {e}. Resposta bruta recebida: {resposta_texto}")
        raise
    except Exception as e:
        logging.error(f"Erro gen√©rico inesperado ao interpretar resposta: {e}. Resposta bruta recebida: {resposta_texto}")
        raise
    
@retry(
    wait=wait_exponential(multiplier=1, min=4, max=10), # Espera 4s, 8s, 16s... at√© 10s max
    stop=stop_after_attempt(5), # Tenta 5 vezes
    reraise=True, # Se quiser que a exce√ß√£o seja propagada ap√≥s todas as tentativas
    before_sleep=log_custom_before_sleep
)

def sugerir_substituicoes(genai_model, texto_cv, requisitos_vaga, model="gemini-1.5-flash"):
    """
    Gera sugest√µes de substitui√ß√£o de termos no CV usando a API do Gemini.
    """
    prompt = f"""
Voc√™ √© um especialista em RH e otimiza√ß√£o de curr√≠culos. Analise o curr√≠culo e os requisitos da vaga abaixo.
Sugira substitui√ß√µes de termos no curr√≠culo para que ele se alinhe melhor aos requisitos da vaga.
Concentre-se em trocar jarg√µes internos ou termos menos comuns por palavras-chave presentes na descri√ß√£o da vaga ou mais reconhecidas no mercado.

**Formato da Resposta:**
Sua resposta deve ser ESTRITAMENTE um array JSON v√°lido (uma lista de objetos JSON). Cada objeto deve conter duas chaves: "original" (o termo do CV) e "substituto" (a sugest√£o).
N√ÉO inclua nenhuma explica√ß√£o, texto introdut√≥rio ou formata√ß√£o extra. Apenas o array JSON.
Seja sucinto e pr√°tico, n√£o sugira mais do que 4 itens!

### Exemplo de Sa√≠da Esperada:
[
  {{"original": "Termo Antigo 1", "substituto": "Termo Novo 1"}},
  {{"original": "Termo Antigo 2", "substituto": "Termo Novo 2"}}
]

-----------------
{texto_cv}
-----------------
Compare com estes requisitos da vaga:
{requisitos_vaga}
-----------------
"""
    try:
        logger.info(f"Enviando prompt para a IA (modelo: {model})...")
        modelo = genai_model.GenerativeModel(model)
        config = genai.types.GenerationConfig(temperature=0.7, response_mime_type="application/json")
        response = modelo.generate_content(
            prompt,
            generation_config=config
        )
        logger.info(f"DEBUG: repr(response.text) antes de interpretar:\n {repr(response.text)}")

        sugestoes_ia = interpretar_resposta_ia(response.text)
        logger.info(f"DEBUG: Sugest√µes interpretadas da IA: {sugestoes_ia}")
        return sugestoes_ia
    
    except Exception as e:
        logger.error(f"Ocorreu um erro ao chamar a API: {e}")
        logger.exception("Detalhes do erro na fun√ß√£o sugerir_substituicoes")
        raise

def extrair_texto_docx(caminho_arquivo):
    """Extrai texto de um arquivo DOCX."""
    try:
        doc = Document(caminho_arquivo)
        texto = []
        for p in doc.paragraphs:
            texto.append(p.text)
        return "\n".join(texto)
    except Exception as e:
        logger.error(f"Erro ao extrair texto do DOCX '{caminho_arquivo}': {e}")
        sys.exit(1)

# ================= FLUXO PRINCIPAL =================

def main():
    load_dotenv()
    genai_instance = carrega_chave()
    all_vaga_suggestions = []

    try:
        # Carregar informa√ß√µes das configura√ß√µes (para obter o caminho do CV)
        config_path = os.environ.get('CONFIG_JSON_PATH', 'configs/linkedin.json')
        with open(config_path, "r", encoding="utf-8") as file:
            linkedin_config = json.load(file)
    except Exception as e:
        logger.error(f"Erro ao carregar configura√ß√£o: {e}")
        sys.exit(1)

    diretorio_cv = linkedin_config["input_file_cv"]

    # L√™ as vagas (analisadas) do JSON (entrada via stdin ou arquivo)
    try:
        if stdin_has_data():
            entrada_json = sys.stdin.read().strip()
            logger.info("Lido JSON do stdin.")
        elif len(sys.argv) > 1:
            with open(sys.argv[1]) as f:
                entrada_json = f.read()
            logger.info(f"Lido JSON do arquivo {sys.argv[1]}.")
        else:
            logger.info("Nenhuma entrada de vagas fornecida! Use argumento de arquivo ou STDIN.")
            sys.exit(1)
        vagas_analisadas = json.loads(entrada_json)
        if not isinstance(vagas_analisadas, list):
            # Se for um √∫nico objeto, transforme em lista para processamento
            vagas_analisadas = [vagas_analisadas]

    except Exception as e:
        logger.error(f"Erro ao ler entrada de vagas: {e}")
        sys.exit(1)

    texto_cv = extrair_texto_docx(diretorio_cv)

    for vaga_dict in vagas_analisadas:
        analise = vaga_dict.get("analise", {})
        referencia = vaga_dict.get("referencia", {})

        # Extrair c√≥digo da vaga de forma robusta
        code = referencia.get("Code", "SEM_CODIGO")
        codigo_vaga = str(int(float(code))) if isinstance(code, (int, float, str)) and str(code).isdigit() else str(code)

        logger.info(f"Iniciando gera√ß√£o de sugest√µes para a vaga: {codigo_vaga}")

        linhas_requisitos = []
        campos_lista = [
            "requisitos_obrigatorios",
            "requisitos_desejaveis",
            "soft_skills",
            "hard_skills"
        ]

        for campo in campos_lista:
            if campo in analise and isinstance(analise[campo], list):
                if analise[campo]: # Adiciona apenas se a lista n√£o estiver vazia
                    linhas_requisitos.append(f"{campo.replace('_',' ').capitalize()}: {', '.join(analise[campo])}")
        
        # Adiciona outros campos que n√£o s√£o listas e n√£o foram processados acima
        for campo, valor in analise.items():
            if campo not in campos_lista and not isinstance(valor, (list, dict)):
                linhas_requisitos.append(f"{campo}: {valor}")
        
        requisitos_texto = "\n".join(linhas_requisitos)

        if not requisitos_texto.strip():
            logger.warning(f"Requisitos de vaga vazios para {codigo_vaga}. Pulando gera√ß√£o de sugest√µes.")
            continue
        
        logger.info(f"Solicitando sugest√µes IA para vaga {codigo_vaga}...")
        sugestoes_ia = sugerir_substituicoes(genai_instance, texto_cv, requisitos_texto)
        time.sleep(3)
        logger.info(f"Sugest√µes recebidas para {codigo_vaga}: {sugestoes_ia}")

        # Adi√ß√£o da corre√ß√£o manual da idade (se ainda for necess√°ria)
        # Idealmente, isso seria configur√°vel ou parte de um p√≥s-processamento separado
        # if sugestoes_ia is not None:
        #     sugestoes_ia.append({"original": "Brasileira, solteira, 52 anos, sem filhos",
        #                          "substituto": "Brasileira, solteira, 53 anos, sem filhos"})
        #     logger.info(f"Adicionada corre√ß√£o manual da idade. Total de sugest√µes: {len(sugestoes_ia)}")
        # else:
        #     sugestoes_ia = [] # Garante que sugestoes_ia √© uma lista

        all_vaga_suggestions.append({
            "codigo": codigo_vaga,
            "referencia": referencia, # Manter a refer√™ncia completa da vaga
            "sugestoes": sugestoes_ia if sugestoes_ia is not None else []
        })

    # Imprime o JSON consolidado de todas as sugest√µes para o stdout
    print(json.dumps(all_vaga_suggestions, ensure_ascii=False, indent=2))
    logger.info("Processo de gera√ß√£o de sugest√µes finalizado.")

if __name__ == "__main__":
    main()
	
# cv_aplicador.py

import sys
from dotenv import load_dotenv
from docx import Document
import os
import json
import logging
import subprocess
import time
import re # Para regex na interpreta√ß√£o da resposta (se necess√°rio, mas n√£o deve mais ser neste script)

# ================= LOGGING SETUP =================
loglevel = os.environ.get("MY_LOG_LEVEL", "INFO").upper()
logger = logging.getLogger(__name__)
logger.setLevel(loglevel)

if not logger.handlers:
    handler = logging.StreamHandler(sys.stderr)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)

# ================= FUN√á√ïES DO SCRIPT =================

def stdin_has_data():
    """Verifica se h√° dados na entrada padr√£o (stdin)."""
    import select
    if sys.stdin.isatty():
        return False
    rlist, _, _ = select.select([sys.stdin], [], [], 0)
    return bool(rlist)

def criar_diretorio_vaga(base_dir, codigo_vaga):
    """Cria um diret√≥rio para a vaga espec√≠fica."""
    dir_destino = os.path.join(base_dir, str(codigo_vaga))
    os.makedirs(dir_destino, exist_ok=True)
    logger.info(f"Diret√≥rio criado/verificado: '{dir_destino}' para a vaga '{codigo_vaga}'")
    return dir_destino

def gerar_pdf_linux(docx_path, output_dir):
    """Converte um arquivo DOCX para PDF usando LibreOffice (para Linux)."""
    logger.info(f"Iniciando convers√£o de '{os.path.basename(docx_path)}' para PDF...")
    
    caminho_docx_abs = os.path.abspath(docx_path)
    diretorio_saida_abs = os.path.abspath(output_dir)

    if not os.path.exists(caminho_docx_abs):
        logger.error(f"Arquivo DOCX n√£o encontrado em: {caminho_docx_abs}")
        return False

    if not os.path.exists(diretorio_saida_abs):
        logger.error(f"Diret√≥rio de sa√≠da n√£o encontrado em: {diretorio_saida_abs}")
        return False
    
    cmd_list = [    
        "libreoffice",
        "--headless",
        "--nologo",
        "--nodefault",
        "--nolockcheck",
        "--convert-to", "pdf",
        "--outdir", diretorio_saida_abs,
        caminho_docx_abs
    ]
    
    logger.info(f"Executando convers√£o para PDF: {' '.join(cmd_list)}")

    try:
        resultado = subprocess.run(
            cmd_list, 
            check=True, 
            capture_output=True, 
            text=True
        )
        logger.info(f"LibreOffice stdout: {resultado.stdout}")
        logger.info(f"Convers√£o para PDF bem-sucedida para {os.path.basename(docx_path)}")
        # Retorna o caminho completo do PDF gerado
        pdf_filename = os.path.splitext(os.path.basename(docx_path))[0] + ".pdf"
        return os.path.join(diretorio_saida_abs, pdf_filename)

    except FileNotFoundError:
        logger.error("Erro: O comando 'libreoffice' n√£o foi encontrado. Verifique se est√° no PATH do sistema.")
        logger.error("Dica: No Ubuntu/Debian, instale com 'sudo apt-get install libreoffice'.")
        return False
    except subprocess.CalledProcessError as e:
        logger.error("Erro ao executar o comando do LibreOffice.")
        logger.error(f"C√≥digo de Retorno: {e.returncode}")
        logger.error(f"Stdout: {e.stdout}")
        logger.error(f"Stderr: {e.stderr}")
        return False
    except Exception as e:
        logger.error(f"Erro inesperado durante a convers√£o para PDF: {e}")
        return False

def extrair_texto_docx(caminho_arquivo):
    """Extrai texto de um arquivo DOCX (fun√ß√£o auxiliar, pode ser redundante se o CV j√° foi lido antes)."""
    try:
        doc = Document(caminho_arquivo)
        texto = []
        for p in doc.paragraphs:
            texto.append(p.text)
        return "\n".join(texto)
    except Exception as e:
        logger.error(f"Erro ao extrair texto do DOCX '{caminho_arquivo}': {e}")
        # N√£o sys.exit(1) aqui, pois pode ser que o arquivo esteja corrompido, mas queremos continuar
        # processando outras vagas, se houver.
        return None

def substituir_texto_docx(caminho_arquivo_original, substituicoes):
    """
    Realiza as substitui√ß√µes no documento DOCX, preservando a formata√ß√£o.
    Retorna o objeto Document modificado ou None em caso de erro.
    """
    try:
        document = Document(caminho_arquivo_original)
        logger.info(f"Documento '{os.path.basename(caminho_arquivo_original)}' carregado para substitui√ß√£o.")

        # Combina o processamento para par√°grafos e tabelas
        todos_paragrafos = list(document.paragraphs)
        for table in document.tables:
            for row in table.rows:
                for cell in row.cells:
                    todos_paragrafos.extend(cell.paragraphs)

        for p in todos_paragrafos:
            for sub in substituicoes:
                original = str(sub.get("original", ""))
                substituto = str(sub.get("substituto", ""))

                if not original or not substituto:
                    logger.warning(f"Sugest√£o de substitui√ß√£o inv√°lida: {sub}. Ignorando.")
                    continue

                # 1. Junta todas as 'runs' para encontrar o texto completo no par√°grafo
                texto_completo_paragrafo = ''.join(run.text for run in p.runs)

                # 2. Se o texto a ser substitu√≠do existe no par√°grafo completo...
                if original in texto_completo_paragrafo:
                    logger.info(f"Encontrado '{original}' no par√°grafo. Realizando substitui√ß√£o.")
                    
                    # Para substituir preservando formata√ß√£o, precisamos reconstruir o par√°grafo.
                    # A maneira mais simples (e que o c√≥digo original j√° tentava) √© recriar o par√°grafo
                    # ou manipular as runs diretamente. A abordagem de Python-Docx para isso √© complexa
                    # se o termo original estiver quebrado entre runs.
                    # Uma alternativa pragm√°tica √© substituir o texto completo e re-aplicar uma formata√ß√£o padr√£o
                    # ou tentar copiar a formata√ß√£o da primeira run.
                    
                    # Abordagem simplificada (pode perder formata√ß√£o granular em certos casos complexos):
                    # Se o par√°grafo tiver apenas uma run, ou se for simples, essa abordagem funciona bem.
                    if len(p.runs) == 1 and original in p.runs[0].text:
                        p.runs[0].text = p.runs[0].text.replace(original, substituto)
                        logger.info(f"Substitui√ß√£o direta em run √∫nica: '{original}' por '{substituto}'.")
                    else:
                        # Para casos mais complexos, onde o texto est√° em v√°rias runs,
                        # √© mais seguro substituir o texto completo e, em seguida,
                        # limpar as runs existentes e adicionar uma nova run com o texto substitu√≠do.
                        # Isso garante que a substitui√ß√£o ocorra, mas pode redefinir o estilo
                        # de todo o par√°grafo para o estilo padr√£o, a menos que seja re-aplicado.

                        # Salva o estilo do par√°grafo antes de limpar
                        paragrafo_style = p.style

                        # Limpa as runs existentes (remove todo o texto)
                        for run in p.runs:
                            run.text = ""
                        
                        # Adiciona uma nova run com o texto substitu√≠do
                        new_text = texto_completo_paragrafo.replace(original, substituto)
                        p.add_run(new_text)
                        
                        # Tenta restaurar o estilo do par√°grafo
                        p.style = paragrafo_style
                        
                        logger.info(f"Substitui√ß√£o complexa em m√∫ltiplas runs: '{original}' por '{substituto}'.")
        
        return document

    except Exception as e:
        logger.error(f"Erro ao processar o documento '{caminho_arquivo_original}' para substitui√ß√£o: {e}")
        return None

# ================= FLUXO PRINCIPAL =================

def main():
    load_dotenv()
    processing_results = []

    try:
        # Carregar informa√ß√µes das configura√ß√µes
        config_path = os.environ.get('CONFIG_JSON_PATH', 'configs/linkedin.json')
        with open(config_path, "r", encoding="utf-8") as file:
            linkedin_config = json.load(file)
    except Exception as e:
        logger.error(f"Erro ao carregar configura√ß√£o: {e}")
        sys.exit(1)

    diretorio_cv_original = linkedin_config["input_file_cv"]
    output_base_dir = linkedin_config['output_dir']

    # L√™ o JSON consolidado de sugest√µes (entrada via stdin ou arquivo)
    try:
        if stdin_has_data():
            entrada_json = sys.stdin.read().strip()
            logger.info("Lido JSON de sugest√µes do stdin.")
        elif len(sys.argv) > 1:
            with open(sys.argv[1]) as f:
                entrada_json = f.read()
            logger.info(f"Lido JSON de sugest√µes do arquivo {sys.argv[1]}.")
        else:
            logger.info("Nenhuma entrada de sugest√µes fornecida! Use argumento de arquivo ou STDIN.")
            sys.exit(1)
        
        all_vaga_suggestions = json.loads(entrada_json)
        if not isinstance(all_vaga_suggestions, list):
            # Se for um √∫nico objeto, transforme em lista para processamento
            all_vaga_suggestions = [all_vaga_suggestions]

    except Exception as e:
        logger.error(f"Erro ao ler entrada de sugest√µes: {e}")
        sys.exit(1)
    
    if not os.path.exists(diretorio_cv_original):
        logger.error(f"Arquivo do CV original n√£o encontrado: {diretorio_cv_original}")
        sys.exit(1)

    # Itera sobre cada vaga e suas sugest√µes
    for vaga_data in all_vaga_suggestions:
        codigo_vaga = vaga_data.get("codigo", "SEM_CODIGO")
        sugestoes = vaga_data.get("sugestoes", [])
        referencia = vaga_data.get("referencia", {})

        logger.info(f"\nProcessando documentos para a vaga: {codigo_vaga}")
        
        dir_vaga = criar_diretorio_vaga(output_base_dir, codigo_vaga)
        novo_docx_path = os.path.join(dir_vaga, f"CV_Modificado_{codigo_vaga}.docx")

        if not sugestoes:
            logger.warning(f"Nenhuma sugest√£o de otimiza√ß√£o para a vaga {codigo_vaga}. Pulando modifica√ß√£o do DOCX.")
            # Opcional: Copiar o CV original para o diret√≥rio da vaga mesmo sem modifica√ß√µes
            # import shutil
            # shutil.copy(diretorio_cv_original, novo_docx_path)
            # logger.info(f"Copiado CV original para {novo_docx_path} (sem modifica√ß√µes).")
            processing_results.append({
                "codigo": codigo_vaga,
                "status": "Nenhuma sugest√£o, DOCX original n√£o modificado",
                "output_dir": dir_vaga,
                "referencia": referencia
            })
            continue # Passa para a pr√≥xima vaga

        # Aplicar sugest√µes e salvar novo DOCX
        sucesso_docx_obj = substituir_texto_docx(diretorio_cv_original, sugestoes)

        if sucesso_docx_obj:
            try:
                sucesso_docx_obj.save(novo_docx_path)
                logger.info(f"Documento modificado salvo como '{os.path.basename(novo_docx_path)}'.")
           
                # Gerar PDF
                caminho_pdf_gerado = gerar_pdf_linux(novo_docx_path, dir_vaga)
                if caminho_pdf_gerado:
                    logger.info(f"PDF gerado com sucesso em: {caminho_pdf_gerado}")
                    processing_results.append({
                        "codigo": codigo_vaga,
                        "status": "Sucesso",
                        "output_docx": novo_docx_path,
                        "output_pdf": caminho_pdf_gerado,
                        "output_dir": dir_vaga,
                        "sugestoes_aplicadas": sugestoes,
                        "referencia": referencia
                    })
                else:
                    logger.error(f"Falha ao gerar PDF para a vaga {codigo_vaga}.")
                    processing_results.append({
                        "codigo": codigo_vaga,
                        "status": "Falha ao gerar PDF",
                        "output_docx": novo_docx_path,
                        "output_dir": dir_vaga,
                        "sugestoes_aplicadas": sugestoes,
                        "referencia": referencia
                    })
            except Exception as e:
                logger.error(f"Falha ao tentar salvar o DOCX modificado para a vaga {codigo_vaga}: {e}")
                processing_results.append({
                    "codigo": codigo_vaga,
                    "status": "Falha ao salvar DOCX modificado",
                    "output_dir": dir_vaga,
                    "erro": str(e),
                    "referencia": referencia
                })
        else:
            logger.error(f"Falha ao modificar o arquivo DOCX para a vaga {codigo_vaga}. Nenhum arquivo salvo.")
            processing_results.append({
                "codigo": codigo_vaga,
                "status": "Falha ao modificar DOCX",
                "output_dir": dir_vaga,
                "referencia": referencia
            })
        
        time.sleep(0.5) # Pequeno atraso para evitar sobrecarga de I/O ou CPU

    # Imprime o JSON de resultados de processamento para o stdout
    print(json.dumps(processing_results, ensure_ascii=False, indent=2))
    logger.info("Processo de aplica√ß√£o de sugest√µes e gera√ß√£o de documentos finalizado.")

if __name__ == "__main__":
    main()
	
#!/bin/bash
#run_linkedin.sh
export MY_LOG_LEVEL=WARNING
cd /data/linkedin-automacao
source venv/bin/activate
venv/bin/python3 scripts/analise_vaga_ia.py 2> logs/analise_vagas_logs.txt

#!/bin/bash
#run_cd_aplicador.sh
echo -e "\n==== Inicio $(date) ====" 2>> /data/linkedin-automacao/logs/cv_aplicador.txt

#whoami 2>&1
#id 2>&1

#export HOME=/data/linkedin-automacao/tmp_home_1002
export MY_LOG_LEVEL=WARNING

cd /data/linkedin-automacao 2>&1

source venv/bin/activate 2>&1
#export GOOGLE_APPLICATION_CREDENTIALS="application_default_credentials.json"

venv/bin/python3 scripts/cv_aplicador.py 2>&1

#chown -R 1002:1002 output 2>&1
#chmod -R 775 output 2>&1

echo -e "\n==== Fim $(date) ====" 2>> logs/cv_aplicador.txt

#!/bin/bash
#run_cv_sugestor.sh
echo -e "\n==== Inicio $(date) ====" 2>> /data/linkedin-automacao/logs/cv_logs.txt

#whoami 2>&1
#id 2>&1

#export HOME=/data/linkedin-automacao/tmp_home_1002
export MY_LOG_LEVEL=INFO

cd /data/linkedin-automacao 2>&1

source venv/bin/activate 2>&1
export GOOGLE_APPLICATION_CREDENTIALS="application_default_credentials.json"

venv/bin/python3 scripts/cv_sugestor.py 2>&1

#chown -R 1002:1002 output 2>&1
#chmod -R 775 output 2>&1

echo -e "\n==== Fim $(date) ====" 2>> logs/cv_logs.txt

#!/bin/bash
#run_aderencia.sh
export MY_LOG_LEVEL=WARNING
cd /data/linkedin-automacao
source venv/bin/activate
export GOOGLE_APPLICATION_CREDENTIALS="application_default_credentials.json"
venv/bin/python3 scripts/aderencia_cv_vaga_ia.py 2> logs/aderencia_logs.txt